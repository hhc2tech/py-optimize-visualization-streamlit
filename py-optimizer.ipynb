{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization visulization\n",
    "#https://www.linkedin.com/feed/update/urn:li:activity:7199237121243570176/?utm_source=share&utm_medium=member_android\n",
    "\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.0.1 Linear regression\n",
    "def Linearregression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred, color='blue', linewidth=3, label='Linear Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Linear Regression')\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred, mode='markers',\n",
    "                                  name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred, z=y_pred,\n",
    "                                  mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y',\n",
    "                                    zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Linear Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.0.2 lasso regression\n",
    "def lasso(X, y, alpha):\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X, y)\n",
    "    y_pred_lasso = lasso_model.predict(X)\n",
    "    coef = lasso_model.coef_[0]\n",
    "    intercept = lasso_model.intercept_\n",
    "    formula = f'y = {coef:.2f}X + {intercept:.2f}'\n",
    "    explanation = f\"In Lasso regression, the penalty term (alpha) is added to the absolute values of the coefficients (L1 regularization), which can result in sparse models with some coefficients being exactly zero.\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred_lasso, color='red', linewidth=2, label='Lasso Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    st.title(f'Lasso Regression')\n",
    "    st.write(f\"**Formula:** {formula}\")\n",
    "    st.write(f\"**Explanation:** {explanation}\")\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred_lasso, mode='markers', name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred_lasso, z=y_pred_lasso, mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y',zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Lasso Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.0.3 ridge\n",
    "def ridge(X, y, alpha):\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X, y)\n",
    "    y_pred_ridge = ridge_model.predict(X)\n",
    "    coef = ridge_model.coef_[0]\n",
    "    intercept = ridge_model.intercept_\n",
    "    formula = f'y = {coef:.2f}X + {intercept:.2f}'\n",
    "    explanation = f\"In Ridge regression, the penalty term (alpha) is added to the square of the coefficients (L2 regularization), which helps in reducing the complexity of the model.\"\n",
    "    color = 'green' if alpha < 1 else 'blue' # Change color based on alpha value\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred_ridge, color=color, linewidth=2, label='Ridge Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    st.title(f'Ridge Regression')\n",
    "    st.write(f\"**Formula:** {formula}\")\n",
    "    st.write(f\"**Explanation:** {explanation}\")\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred_ridge,mode='markers', name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred_ridge,z=y_pred_ridge, mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y',zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Ridge Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 200).reshape(-1, 1)\n",
    "y = 2 * X.squeeze() + np.random.normal(0, 2, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 01:21:03.370 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/hha/miniconda3/envs/sklearn39/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# 1.0.4 Streamlit interface\n",
    "st.sidebar.header('Regression Type')\n",
    "regression_type = st.sidebar.selectbox('Select Regression Type', ('Linear Regression', 'Lasso Regression', 'Ridge Regression'))\n",
    "if regression_type == 'Linear Regression':\n",
    "    Linearregression(X, y)\n",
    "elif regression_type == 'Lasso Regression':\n",
    "    alpha = st.sidebar.slider('Select Alpha', min_value=0.1, max_value=10.0, step=0.1)\n",
    "    lasso(X, y, alpha)\n",
    "else: # Ridge Regression\n",
    "    alpha = st.sidebar.slider('Select Alpha', min_value=0.1, max_value=100.0, step=0.5)\n",
    "    ridge(X, y, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Gradient Descent Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "num_samples = 100\n",
    "x1 = np.random.uniform(0, 10, num_samples)\n",
    "x2 = np.random.uniform(0, 10, num_samples)\n",
    "intercept = np.ones(num_samples)\n",
    "error_term = np.random.normal(0, 0.5, num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_true = 1.5\n",
    "coef1_true = 2\n",
    "coef2_true = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = intercept_true * intercept + coef1_true * x1 + coef2_true * x2 + error_term\n",
    "feature_matrix = np.vstack([intercept, x1, x2]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.0.1 True coefficients\n",
    "true_coefficients = [intercept_true, coef1_true, coef2_true]\n",
    "#2.0.2 Gradient Descent Function\n",
    "def perform_gradient_descent(learning_rate, iterations, initial_coefficients):\n",
    "    coefficients = initial_coefficients\n",
    "    coefficients_history = np.zeros((iterations + 1, 3))\n",
    "    coefficients_history[0, :] = initial_coefficients\n",
    "    loss_history = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        predictions = np.dot(feature_matrix, coefficients)\n",
    "        residuals = y - predictions\n",
    "        gradient = -2 * np.dot(feature_matrix.T, residuals) / num_samples\n",
    "        coefficients = coefficients - learning_rate * gradient\n",
    "        coefficients_history[i + 1, :] = coefficients\n",
    "        loss_history[i] = np.mean(residuals ** 2)\n",
    "    return coefficients_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.0.3 Plotting Functions\n",
    "def plot_parameters_trajectory(coefficients_history, i, j, axis):\n",
    "    axis.plot(true_coefficients[i], true_coefficients[j], marker='p',markersize=10, color='red', label='True Coefficients')\n",
    "    axis.plot(coefficients_history[:, i], coefficients_history[:, j],linestyle='--', marker='o', markersize=5, label='Gradient Descent Path')\n",
    "    axis.set_xlabel(f'Coefficient {i}')\n",
    "    axis.set_ylabel(f'Coefficient {j}')\n",
    "    axis.legend()\n",
    "    axis.grid(True)\n",
    "def plot_gradient_descent(coefficients_history, loss_history, learning_rate,iterations):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    fig.suptitle(f'Gradient Descent Visualization\\nLearning Rate:{learning_rate}, Iterations: {iterations}', fontsize=16)\n",
    "    plot_parameters_trajectory(coefficients_history, 0, 1, axes[0, 0])\n",
    "    plot_parameters_trajectory(coefficients_history, 0, 2, axes[0, 1])\n",
    "    plot_parameters_trajectory(coefficients_history, 1, 2, axes[1, 0])\n",
    "    axes[1, 1].plot(loss_history, label='Loss Function')\n",
    "    axes[1, 1].set_xlabel('Iterations')\n",
    "    axes[1, 1].set_ylabel('Mean Squared Error')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.0.4 Function to plot the loss function with derivatives and minima/maxima\n",
    "def plot_loss_function(loss_history):\n",
    "    iterations = np.arange(len(loss_history))\n",
    "    derivatives = np.gradient(loss_history)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(iterations, loss_history, label='Loss Function', color='blue')\n",
    "    ax.plot(iterations, derivatives, label='Derivative', color='orange')\n",
    "    global_min = np.min(loss_history)\n",
    "    global_min_index = np.argmin(loss_history)\n",
    "    global_max = np.max(loss_history)\n",
    "    global_max_index = np.argmax(loss_history)\n",
    "    ax.plot(global_min_index, global_min, 'go', label='Global Minima')\n",
    "    ax.plot(global_max_index, global_max, 'ro', label='Global Maxima')\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.0.5 Streamlit Interface\n",
    "st.title(\"Interactive Gradient Descent Visualization\")\n",
    "learning_rate = st.sidebar.slider('Learning Rate', min_value=1e-5,max_value=1e-1, value=1e-3, step=1e-5, format=\"%.5f\")\n",
    "iterations = st.sidebar.slider('Number of Iterations', min_value=100,max_value=5000, value=1000, step=100)\n",
    "initial_coefficients = np.array([3.0, 3.0, 3.0])\n",
    "coefficients_history, loss_history = perform_gradient_descent(learning_rate,iterations, initial_coefficients)\n",
    "plot_gradient_descent(coefficients_history, loss_history, learning_rate,iterations)\n",
    "plot_loss_function(loss_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
